# Tutorial: Vision-Language Models (VLMs) — From Absolute Zero

> **Prerequisites**: None. This tutorial is fully self-contained. If you've read the [VLA tutorial](tutorial-vla-from-scratch.md), this deepens your understanding of the perception backbone inside it.

A **Vision-Language Model (VLM)** is a neural network that **understands both images and text together**. It's the "brain" behind ChatGPT-with-images, Google Lens, and — most relevant to us — the perception backbone inside every modern robot.

This tutorial explains how VLMs work from first principles, using real code from our SimScaleAI project.

---

## Part 1: What Is a VLM and Why Should You Care?

### The Problem

Traditional AI systems are specialists:
- **CNNs** (Convolutional Neural Networks) understand images but can't read text
- **LLMs** (Large Language Models like GPT) understand text but are blind
- **Neither** can answer: "Is the red block near the green target?" while looking at a camera feed

### The Solution: VLM

A VLM processes **both** images and text in a unified model. It can:

```
Image of a table + "What color is the block?" → "The block is red"
Image of a robot arm + "Is the gripper open?" → "Yes, the gripper is open"
Image + "Describe what you see" → "A red block on a wooden table with a green target marker"
```

### Where VLMs Fit in the AI Hierarchy

```
                    ┌─────────────────────┐
                    │  Foundation Models   │  (the big picture)
                    └──────────┬──────────┘
                               │
            ┌──────────────────┼──────────────────┐
            │                  │                   │
     ┌──────┴──────┐   ┌──────┴──────┐    ┌──────┴──────┐
     │     LLM     │   │     VLM     │    │     VLA     │
     │ (text only) │   │(image+text) │    │(image+text  │
     │             │   │             │    │  +actions)  │
     │  GPT, LLaMA │   │ CLIP, GPT-4V│    │ RT-2, OpenVLA│
     └─────────────┘   └─────────────┘    └─────────────┘
                              │                   │
                              │    VLM is the     │
                              └──── perception ───┘
                                   backbone of VLA
```

**Key insight**: A VLA (Vision-Language-Action model) is just a **VLM with an action head bolted on**. If you understand VLMs, you understand 80% of VLA.

### Real-World VLMs

| Model | Creator | Parameters | What It Does |
|-------|---------|-----------|--------------|
| **CLIP** | OpenAI | 400M | Matches images to text descriptions |
| **GPT-4V** | OpenAI | ~1.8T | Answers questions about images |
| **Gemini** | Google | ~1T | Multimodal understanding + generation |
| **LLaVA** | UW-Madison | 7-13B | Open-source visual chat |
| **SigLIP** | Google | 400M | CLIP successor, used in OpenVLA |
| **PaLI** | Google | 17B | Used as VLM backbone in RT-2 |

---

## Part 2: The Two Fundamental VLM Architectures

There are two main ways to build a VLM. Understanding both is essential.

### Architecture A: Dual-Encoder (CLIP-style)

**Separate** encoders for image and text, then **compare** their outputs.

```
Image ──► Vision Encoder ──► image_vector (512-dim)
                                                        ──► Similarity Score
Text  ──► Text Encoder   ──► text_vector  (512-dim)

"Is this image about a red block?" → cosine_similarity(image_vec, text_vec) → 0.87
```

**How it's trained**: Show millions of (image, caption) pairs. Push matching pairs close together in vector space, push non-matching pairs apart.

```
Training pair: (photo of a dog, "a golden retriever playing fetch")
  → image_vec and text_vec should be CLOSE (high similarity)

Non-pair: (photo of a dog, "a red sports car")
  → image_vec and text_vec should be FAR (low similarity)
```

**Strengths**: Fast retrieval, good embeddings, good for classification
**Weaknesses**: Can't generate text, can't have conversations

**Used in robotics**: CLIP/SigLIP is the standard vision backbone in VLA models. RT-2 and OpenVLA both use it.

### Architecture B: Generative (GPT-4V-style)

One unified model that **generates text** conditioned on an image.

```
Image ──► Vision Encoder ──► visual tokens ──┐
                                              ├──► LLM Decoder ──► "The red block is..."
Text: "Describe the scene" ──► text tokens ──┘
```

**How it's trained**: Next-token prediction conditioned on the image. The model learns to generate descriptive captions, answer questions, and follow instructions.

**Strengths**: Can converse, reason, and generate detailed descriptions
**Weaknesses**: Slower (autoregressive generation), larger

**Used in robotics**: When the robot needs to reason about what to do before acting. RT-2 uses this approach — the "actions" are just special tokens generated by the LLM.

### Our Project: We Use Architecture A

Our SimScaleAI VLA contains a **dual-encoder VLM** (vision encoder + language encoder) as its perception backbone:

```python
# Inside VisionLanguageAction model (simscaleai/models/vla.py)

self.vision_encoder = VisionEncoder(...)    # Architecture A: Vision side
self.language_encoder = SimpleLanguageEncoder(...)  # Architecture A: Language side
self.fusion_transformer = ...              # Fuses both (goes beyond basic CLIP)
self.action_head = ...                     # VLA extension (not part of pure VLM)
```

The VLM components are the vision encoder + language encoder. Everything after that (fusion + action head) is the VLA extension.

---

## Part 3: The Vision Encoder — How AI "Sees"

### From Pixels to Meaning

A camera captures a 64×64 RGB image — that's 64 × 64 × 3 = 12,288 raw numbers. Most of these numbers are noise (lighting, shadows, textures). The vision encoder's job: compress 12,288 numbers into a small set of **meaningful** vectors.

### Step 1: Patch Embedding — Cutting the Image Into Tiles

Instead of processing individual pixels (too expensive), we cut the image into patches:

```
64×64 image with patch_size=8:

┌────┬────┬────┬────┬────┬────┬────┬────┐
│ P0 │ P1 │ P2 │ P3 │ P4 │ P5 │ P6 │ P7 │
├────┼────┼────┼────┼────┼────┼────┼────┤
│ P8 │ P9 │P10 │P11 │P12 │P13 │P14 │P15 │
├────┼────┼────┼────┼────┼────┼────┼────┤
│P16 │P17 │P18 │P19 │P20 │P21 │P22 │P23 │
├────┼────┼────┼────┼────┼────┼────┼────┤
│P24 │P25 │P26 │P27 │P28 │P29 │P30 │P31 │
├────┼────┼────┼────┼────┼────┼────┼────┤
│P32 │P33 │P34 │P35 │P36 │P37 │P38 │P39 │
├────┼────┼────┼────┼────┼────┼────┼────┤
│P40 │P41 │P42 │P43 │P44 │P45 │P46 │P47 │
├────┼────┼────┼────┼────┼────┼────┼────┤
│P48 │P49 │P50 │P51 │P52 │P53 │P54 │P55 │
├────┼────┼────┼────┼────┼────┼────┼────┤
│P56 │P57 │P58 │P59 │P60 │P61 │P62 │P63 │
└────┴────┴────┴────┴────┴────┴────┴────┘

64 patches, each 8×8 pixels × 3 colors = 192 numbers per patch
```

Each patch is projected to a 128-dimensional vector using a linear layer:

```python
class PatchEmbedding(nn.Module):
    def __init__(self, image_size=64, patch_size=8, embed_dim=128):
        self.num_patches = (64 // 8) ** 2  # = 64

        # Conv2d with kernel_size=stride=patch_size acts as a patch projector
        # It slides an 8×8 window across the image with stride 8 (no overlap)
        # Each position produces a 128-dim output
        self.proj = nn.Conv2d(3, 128, kernel_size=8, stride=8)

        # Each patch needs to know WHERE it is in the image
        # Top-left patch should behave differently from bottom-right
        self.pos_embed = nn.Parameter(torch.randn(1, 64, 128) * 0.02)

    def forward(self, image):
        x = self.proj(image)              # (B, 128, 8, 8)
        x = x.flatten(2).transpose(1, 2)  # (B, 64, 128) — 64 patches
        x = x + self.pos_embed            # Add position info
        return x  # (B, 64, 128)
```

**Why patches, not pixels?** Processing all 12,288 pixels individually with self-attention would require 12,288² ≈ 150 million attention computations. With 64 patches, it's only 64² = 4,096 — a 37,000× reduction.

### Step 2: CLS Token — The Summary

We prepend a special **CLS** (classification) token to the patch sequence. After the transformer processes everything, this token serves as a global summary of the entire image.

```python
self.cls_token = nn.Parameter(torch.randn(1, 1, 128) * 0.02)

# In forward:
cls = self.cls_token.expand(batch_size, -1, -1)  # (B, 1, 128)
tokens = torch.cat([cls, patches], dim=1)         # (B, 65, 128)
#                   ^CLS  ^64 patches
```

Why not just average all patches? The CLS token is **learnable** — it learns to aggregate the most task-relevant information, not just a flat average.

### Step 3: Transformer Encoder — Making Patches Talk

Raw patches are isolated — patch #7 (top-right) doesn't know what patch #35 (center) sees. The transformer's self-attention mechanism fixes this:

```python
encoder_layer = nn.TransformerEncoderLayer(
    d_model=128,        # Each token is 128-dim
    nhead=4,            # 4 attention heads (each 32-dim)
    dim_feedforward=512, # MLP expansion factor 4×
    dropout=0.1,
    activation="gelu",
    batch_first=True,
    norm_first=True,    # Pre-LayerNorm (more stable training)
)
self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)
```

After 2 layers of self-attention:
- Patch #7 (showing part of the red block) now has information about patches #3, #14, #15 (other parts of the red block) — it understands the **whole object**, not just its 8×8 tile
- CLS token has a high-level summary: "red block in center, green target at top-left, robot arm coming from right"

### Step 4: LayerNorm — Stabilizing the Output

```python
self.norm = nn.LayerNorm(128)

# In forward:
tokens = self.encoder(tokens)
return self.norm(tokens)  # (B, 65, 128) — normalized visual tokens
```

LayerNorm ensures all output values have mean ≈ 0 and std ≈ 1, which stabilizes training.

### The Complete Vision Encoder

```python
class VisionEncoder(nn.Module):
    def __init__(self, image_size=64, patch_size=8, embed_dim=128,
                 num_heads=4, num_layers=2, dropout=0.1):
        self.patch_embed = PatchEmbedding(image_size, patch_size, 3, embed_dim)
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=num_heads,
            dim_feedforward=embed_dim * 4,
            dropout=dropout, activation="gelu",
            batch_first=True, norm_first=True,
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, images):
        patches = self.patch_embed(images)         # (B, 64, 128)
        cls = self.cls_token.expand(B, -1, -1)     # (B, 1, 128)
        tokens = torch.cat([cls, patches], dim=1)  # (B, 65, 128)
        tokens = self.encoder(tokens)              # (B, 65, 128)
        return self.norm(tokens)                   # (B, 65, 128)
```

**Input**: Raw image (B, 3, 64, 64) — 12,288 numbers
**Output**: 65 vectors of 128 dims each — a structured, meaningful representation

---

## Part 4: The Language Encoder — How AI "Reads"

### From Characters to Meaning

A text instruction like `"pick up the red block"` is just a sequence of characters. The language encoder converts this into vectors that the model can reason about.

### Step 1: Tokenization — Text to Numbers

Our project uses a simple **character-level** tokenizer:

```python
def _build_vocab(self):
    chars = " abcdefghijklmnopqrstuvwxyz0123456789.,!?'-"
    self._char_to_idx = {c: i + 1 for i, c in enumerate(chars)}
    # ' ' → 1, 'a' → 2, 'b' → 3, ..., 'z' → 27, ...

def tokenize(self, texts, device):
    # "pick up" → [17, 10, 4, 12, 1, 22, 17]
    # Pad to 64 characters
    # "pick up" + [0]*57 → [17,10,4,12,1,22,17,0,0,...,0]
```

**Production models use better tokenizers:**

| Tokenizer | Level | "pick up the red block" becomes | Used by |
|-----------|-------|--------------------------------|---------|
| **Our char-level** | Character | `[17,10,4,12,1,22,17,1,21,9,6,1,19,6,5,1,3,13,16,4,12]` (21 tokens) | SimScaleAI |
| **BPE (byte-pair)** | Subword | `['pick', ' up', ' the', ' red', ' block']` (5 tokens) | GPT, CLIP |
| **SentencePiece** | Subword | `['▁pick', '▁up', '▁the', '▁red', '▁block']` (5 tokens) | LLaMA, PaLM |

Subword tokenizers are more efficient (fewer tokens) and capture word meaning better, but character-level works for demonstrating the architecture.

### Step 2: Token Embedding — Numbers to Vectors

Each token index looks up a learned vector in an embedding table:

```python
self.token_embed = nn.Embedding(vocab_size=1000, embedding_dim=128)

# token_ids: [17, 10, 4, 12, 1, 22, 17, 0, 0, ..., 0]  (64 tokens)
# After embedding: (64, 128) — each char is now a 128-dim vector

# The embedding table is LEARNED during training
# 'p' (17) might map to [0.3, -0.1, 0.8, ...] (128 numbers)
# 'i' (10) might map to [0.1,  0.4, -0.2, ...] (128 numbers)
# Characters in similar contexts develop similar embeddings
```

### Step 3: Positional Embedding — Order Matters

"pick up" and "up pick" use the same characters but mean different things. Position embeddings encode order:

```python
self.pos_embed = nn.Parameter(torch.randn(1, 64, 128) * 0.02)

# Position 0 adds one learned vector
# Position 1 adds a different learned vector
# Position 63 adds yet another learned vector
# Result: "p" at position 0 has a different representation than "p" at position 5
```

### Step 4: Transformer — Characters Learn Context

The self-attention transformer lets each character attend to every other character:

```python
encoder_layer = nn.TransformerEncoderLayer(
    d_model=128, nhead=4,
    dim_feedforward=512,
    dropout=0.1, activation="gelu",
    batch_first=True, norm_first=True,
)
self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)
```

Before transformer:
- Token 'r' knows only that it's the letter 'r' at position 14
- Token 'e' knows only that it's the letter 'e' at position 15

After transformer:
- Token 'r' at position 14 now knows it's part of "red" which comes after "the" — it represents the concept "a color descriptor"
- Token 'e' at position 15 knows it's part of "red" and that "red" modifies "block" — it carries the meaning "the specified object is red"

### The Complete Language Encoder

```python
class SimpleLanguageEncoder(nn.Module):
    def __init__(self, vocab_size=1000, max_len=64, embed_dim=128,
                 num_heads=4, num_layers=2, dropout=0.1):
        self.token_embed = nn.Embedding(vocab_size, embed_dim)
        self.pos_embed = nn.Parameter(torch.randn(1, max_len, embed_dim) * 0.02)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=num_heads,
            dim_feedforward=embed_dim * 4, dropout=dropout,
            activation="gelu", batch_first=True, norm_first=True,
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, token_ids):
        x = self.token_embed(token_ids) + self.pos_embed[:, :seq_len]
        x = self.encoder(x)
        return self.norm(x)  # (B, 64, 128)
```

**Input**: Token IDs (B, 64) — padded character indices
**Output**: 64 vectors of 128 dims each — contextualized representations of each character

---

## Part 5: Fusion — Where Vision Meets Language

### The Core Challenge of VLMs

You have two sets of vectors:
- Visual tokens: 65 × 128 (image understanding)
- Language tokens: 64 × 128 (text understanding)

They live in the **same dimensional space** (128-dim), but they were computed independently. Neither knows about the other. Fusion is the bridge.

### Approach 1: Contrastive (CLIP-style)

Take one vector from each and compute similarity:

```
visual_summary = mean(visual_tokens)   or   CLS_token    → (128,)
text_summary   = mean(language_tokens) or   [CLS] token  → (128,)

similarity = cosine(visual_summary, text_summary)
           = dot(v, t) / (|v| × |t|)
           = ranges from -1 (opposite) to +1 (identical)
```

Training objective (**contrastive loss**):

```
Batch of N image-text pairs:
  im₁ matches text₁  →  similarity should be HIGH
  im₁ vs text₂       →  similarity should be LOW
  im₂ vs text₁       →  similarity should be LOW
  im₂ matches text₂  →  similarity should be HIGH

Loss = -log( exp(sim(im₁,text₁)) / Σ exp(sim(im₁,textⱼ)) )
```

This is how **CLIP** is trained on 400 million image-text pairs from the internet.

### Approach 2: Cross-Attention Fusion (Our Approach)

Instead of a single similarity score, we let **every visual token attend to every language token** and vice versa:

```python
# Concatenate all tokens into one sequence
all_tokens = torch.cat([
    visual_tokens,    # (B, 65, 128) — what the robot sees
    language_tokens,  # (B, 64, 128) — what the human said
    state_token,      # (B, 1, 128)  — robot body state (VLA extension)
], dim=1)             # (B, 130, 128)

# Fusion transformer — cross-modal attention
fused = self.fusion_transformer(all_tokens)  # (B, 130, 128)
```

Inside the fusion transformer, self-attention computes relationships **across modalities**:

```
Visual patch showing RED BLOCK ←→ Language token "red" = HIGH attention
Visual patch showing RED BLOCK ←→ Language token "green" = LOW attention
Visual patch showing GREEN TARGET ←→ Language token "place" = HIGH attention
Visual patch showing EMPTY TABLE ←→ Language token "pick" = LOW attention
```

This is what makes our VLM truly multimodal — it doesn't just **compare** image and text, it **integrates** them at every layer.

### Why This Is Superior to Contrastive Learning

| | CLIP (contrastive) | Fusion Transformer |
|---|---|---|
| Output | Single number (similarity) | Rich token-level representations |
| Knows WHERE in image matches text? | No | Yes — patch-level grounding |
| Can answer "what color is the block?" | Only via ranking | Yes — direct generation |
| Suitable for actions? | No | Yes — CLS token → action head |
| Computation | Cheap (one dot product) | Expensive (full cross-attention) |

For robotics, we need **spatial grounding** (which part of the image is the target?), so fusion is essential.

---

## Part 6: Self-Attention Deep Dive — The Mechanism Behind Everything

Self-attention is the single most important concept in modern AI. Let me break it down completely.

### The Intuition

Imagine you're reading "pick up the **red block** and place it at the green target."

When you read "it," your brain instantly connects "it" → "red block." That's attention. You attended to an earlier part of the sequence to resolve the meaning of the current token.

Self-attention does exactly this, but for every token simultaneously.

### The Math (Simplified)

For each token $x_i$, we compute three vectors:

$$Q_i = W_Q \cdot x_i \quad \text{(Query: "What am I looking for?")}$$
$$K_i = W_K \cdot x_i \quad \text{(Key: "What do I represent?")}$$
$$V_i = W_V \cdot x_i \quad \text{(Value: "What information do I carry?")}$$

Where $W_Q$, $W_K$, $W_V$ are learned weight matrices (128 × 128 in our model).

The attention score between tokens $i$ and $j$:

$$\text{score}_{ij} = \frac{Q_i \cdot K_j}{\sqrt{d_k}}$$

Where $d_k = 128 / 4 = 32$ (embedding dim ÷ number of heads).

After softmax (normalize so scores sum to 1):

$$\alpha_{ij} = \text{softmax}_j(\text{score}_{ij})$$

The output for token $i$:

$$\text{output}_i = \sum_j \alpha_{ij} \cdot V_j$$

Each token's output is a **weighted average** of all tokens' values, where the weights are determined by query-key compatibility.

### Multi-Head Attention — 4 Perspectives

Instead of one big 128-dim attention, we split into 4 heads of 32 dims each:

```
Head 1 (32-dim): might learn to attend to COLOR (red, green, blue)
Head 2 (32-dim): might learn to attend to VERBS (pick, place, lift)
Head 3 (32-dim): might learn to attend to SPATIAL (left, right, near)
Head 4 (32-dim): might learn to attend to OBJECTS (block, target, arm)
```

```python
# PyTorch bundles all this into:
nn.TransformerEncoderLayer(d_model=128, nhead=4, ...)
# Internally: Q, K, V projections + multi-head attention + add & norm + FFN
```

### Concrete Cross-Modal Example

In our model with 130 tokens (65 visual + 64 language + 1 state):

```
Token: Visual patch #27 (shows the red block)
  Query: "I'm a visual feature from the center of the image"

  Attention scores (after softmax):
    Visual patch #26 (adjacent, also red block):  0.15   ← spatial neighbor
    Visual patch #28 (adjacent, table edge):      0.04
    Language token 'r' in "red":                  0.22   ← cross-modal!
    Language token 'e' in "red":                  0.18   ← cross-modal!
    Language token 'd' in "red":                  0.12   ← cross-modal!
    Language token 'b' in "block":                0.08   ← cross-modal!
    Language token ' ' (space):                   0.01
    State token:                                  0.05
    ... all other tokens:                         ~0.15 total

  Output: weighted combination — now this visual token
  "knows" it represents the thing the user called "red block"
```

This cross-modal attention is the **defining feature** of a VLM. It's what makes vision and language truly fused, not just concatenated.

---

## Part 7: How VLMs Are Trained — Three Paradigms

### Paradigm 1: Contrastive Pre-Training (CLIP)

**Goal**: Learn a shared embedding space where matching images and texts are close.

```
Dataset: 400 million (image, caption) pairs from the internet

Batch of 64:
  im₁, text₁ = (photo of dog, "a happy golden retriever")
  im₂, text₂ = (photo of car, "red sports car on highway")
  ...

For each pair (i, j):
  sim(imᵢ, textⱼ) = cosine(vision_enc(imᵢ), text_enc(textⱼ))

Loss: maximize sim(im₁, text₁), minimize sim(im₁, text₂)
  = InfoNCE / contrastive loss

After training:
  vision_enc("photo of cat") ≈ text_enc("a cat sitting on a couch")
  They map to nearby points in 512-dim space!
```

**Scale**: CLIP was trained on 400M image-text pairs using 256 GPUs for weeks. Our project skips this (we'd need vast compute).

### Paradigm 2: Generative Pre-Training (GPT-4V / LLaVA)

**Goal**: Given an image, generate text about it (captions, answers, descriptions).

```
Dataset: Millions of (image, question, answer) triples

Input:  image + "What is shown in this image?"
Target: "A red block on a wooden table next to a green marker"

Model predicts one token at a time:
  "A" → "A red" → "A red block" → ... → "A red block on a wooden table..."

Loss: cross-entropy (next token prediction)
```

### Paradigm 3: Supervised Action Prediction (Our Approach)

**Goal**: Given image + text + state, predict the correct robot action.

```
Dataset: Expert demonstrations with language labels

Input:  image + "pick up the red block" + robot_state
Target: [dx=0.12, dy=-0.20, dz=0.10, gripper=1.0]  (expert's action)

Loss: MSE (mean squared error between predicted and expert actions)
```

This is what happens in our project:

```python
# simscaleai/training/train_vla.py

loss = F.mse_loss(predicted_actions, target_actions)
# predicted: what the model thinks the action should be
# target: what the expert actually did
# MSE: average of (predicted - target)² across all dimensions
```

### Our Training Pipeline

```
1. Generate data:
   Scripted expert → 200 episodes of pick-and-place → HDF5 file

2. Create dataset:
   Load HDF5 + add random language instructions
   38,000 (state, action, language, dummy_image) samples

3. Train VLM+action head (i.e., VLA):
   For 2000 steps:
     sample batch of 32
     forward: image + language + state → predicted_action
     loss = MSE(predicted, expert)
     backward + optimizer step

4. Evaluate:
   Load model → run in simulation → measure reward
```

---

## Part 8: Pre-Training vs Fine-Tuning — The Modern Recipe

### The Two-Phase Approach

Production VLMs follow a two-phase recipe that's crucial to understand:

```
Phase 1: PRE-TRAINING (expensive, done once)
  Generic internet data (billions of image-text pairs)
  Goal: learn general vision + language understanding
  Cost: millions of dollars, weeks of GPU cluster time
  Result: a "foundation model" that understands the visual world

Phase 2: FINE-TUNING (cheap, done per task)
  Task-specific data (thousands of robot demonstrations)
  Goal: adapt the foundation model to YOUR specific robot task
  Cost: a few hundred dollars, hours on a single GPU
  Result: a specialized model for pick-and-place on YOUR robot
```

### Why This Matters for Robotics

```
Without pre-training:
  38,000 robot demos → train VLM from scratch → learns almost nothing
  (not enough data to learn both vision AND language AND actions)

With pre-training:
  400M internet images → CLIP/SigLIP already understands "red block"
  38,000 robot demos → fine-tune action head only → works well!
  (VLM already knows what objects look like, just needs to learn actions)
```

### In Our Project

We train from scratch (no pre-training), which is why our VLA underperforms BC:

```python
# Our model starts with RANDOM weights everywhere:
model = VisionLanguageAction(
    image_size=64, patch_size=8, embed_dim=128,
    num_heads=4, num_layers=2, action_dim=4, state_dim=20,
)
# 1.4M random parameters trying to simultaneously learn:
#   1. How to see (vision encoder) — normally pre-trained
#   2. How to read (language encoder) — normally pre-trained
#   3. How to act (action head) — fine-tuned
# With only 38K samples, it's too much to learn at once
```

A production setup (RT-2 / OpenVLA):

```python
# Start with a pre-trained VLM:
vision_encoder = load_pretrained("google/siglip-400m")   # Already knows vision!
language_model = load_pretrained("meta-llama/Llama-7b")  # Already knows language!

# Only train the action head from scratch:
action_head = nn.Linear(4096, 7)  # Much smaller, easier to train

# Fine-tune everything end-to-end with lower learning rate for pre-trained parts
optimizer = AdamW([
    {"params": vision_encoder.parameters(), "lr": 1e-5},    # Gentle updates
    {"params": language_model.parameters(), "lr": 1e-5},     # Gentle updates
    {"params": action_head.parameters(), "lr": 1e-3},        # Aggressive (new)
])
```

---

## Part 9: Key VLM Concepts Explained

### Concept 1: Embedding Space

An embedding space is a high-dimensional coordinate system where **similar things are near each other**.

```
In a 128-dim embedding space:

  "red block" → [0.3, -0.1, 0.8, 0.2, ...]  (128 numbers)
  "red cube"  → [0.3, -0.1, 0.7, 0.2, ...]  (nearby — similar meaning!)
  "green target" → [-0.5, 0.4, -0.2, 0.9, ...]  (far — different object)

  Image of red block → [0.3, -0.1, 0.8, 0.2, ...]  (near the text "red block"!)
```

CLIP's magic: it creates an embedding space where images and their text descriptions map to the **same region**. That's how it "understands" that an image of a dog matches the text "a photo of a dog."

### Concept 2: Visual Grounding

**Grounding** = connecting language to specific regions of an image.

```
Input: Image + "pick up the red block"

Without grounding:
  Model knows "red block" is mentioned, but not WHERE in the image it is

With grounding (via attention):
  "red block" → high attention on patches #26, #27, #35, #36 (center of image)
  The model knows WHICH pixels correspond to the mentioned object
```

Our fusion transformer does this implicitly via cross-attention between visual and language tokens.

### Concept 3: Zero-Shot Generalization

A well-trained VLM can handle instructions it has **never seen before**:

```
Training data:
  "pick up the red block"
  "grasp the red cube"
  "put the blue ball in the box"

Zero-shot (never seen during training):
  "pick up the blue sphere" → still works!
  
Why? The model learned:
  - "pick up" → reach + grasp motion
  - "blue" → this color in the image
  - "sphere" ≈ "ball" (similar in embedding space)
  It composes these concepts to handle the new instruction
```

This compositionality is the promise of VLMs. Our 1.4M demo model doesn't achieve it (too small, too little data), but RT-2 with 55B parameters does.

### Concept 4: Modality Alignment

The biggest challenge in VLMs: making sure visual and language representations are **compatible**.

```
Problem:
  Vision encoder outputs vectors in "visual space"
  Language encoder outputs vectors in "language space"
  These spaces are initially UNRELATED

Solution (contrastive learning):
  Matching pairs: push visual and language vectors TOGETHER
  Non-matching: push them APART
  After training: both modalities share ONE unified space

Solution (our approach — fusion transformer):
  Concatenate visual + language tokens
  Self-attention lets them interact and align
  After training: the fusion layer learns to bridge the two spaces
```

### Concept 5: Vision Transformer (ViT) vs CNN

Before ViT (2020), everyone used Convolutional Neural Networks (CNNs) for computer vision:

| | CNN | ViT (Vision Transformer) |
|---|---|---|
| Processes | Local regions (3×3 or 5×5 kernels) | Global patches (8×8 or 16×16) |
| Attention | None (fixed receptive field) | Full self-attention (every patch sees every patch) |
| Position | Implicit (convolution structure) | Explicit (positional embeddings) |
| Best at | Spatial hierarchies (edges → objects) | Long-range relationships |
| Integration with language | Awkward (different architecture) | Natural (same transformer architecture!) |
| Used in modern VLMs | ❌ (mostly replaced) | ✅ (standard) |

**Key ViT advantage for VLMs**: Since both vision and language use transformers, they produce the same type of output (sequences of vectors) and can be trivially concatenated and fused. This architectural uniformity is why ViT won.

---

## Part 10: VLMs in the Real World — Production Systems

### Google RT-2 (2023) — The Breakthrough

```
Architecture:
  PaLI (VLM) = ViT-G (2B params) + PaLM (55B params)
  + Action tokens (special vocabulary entries representing motor commands)

How it works:
  Image + "pick up the can" → PaLI generates: "1 128 91 241 5 101 127"
  These 7 numbers ARE the robot action (discretized into 256 bins)

Magic: Actions are just tokens — the LLM treats robot control as language!

Scale:
  Pre-trained on internet data
  Fine-tuned on 130,000 robot demonstrations (13 robots, 700 tasks)
  Can follow instructions it has NEVER seen before
```

### Stanford OpenVLA (2024) — Open Source

```
Architecture:
  SigLIP (VLM) = 400M vision encoder + LLaMA-7B language model
  + Action tokens

How it works:
  Exactly like RT-2, but open-source and fine-tunable
  Released weights + training code on GitHub

Scale:
  Pre-trained SigLIP + LLaMA
  Fine-tuned on Open X-Embodiment (1M+ episodes, 22 robot types)
  7B parameters total
```

### Our SimScaleAI VLA — Demo Scale

```
Architecture:
  Custom ViT (2 layers) + Char-level encoder (2 layers) + Fusion (2 layers)
  + MLP action head

How it works:
  Same architecture as RT-2 / OpenVLA, just much smaller
  No pre-training (all from scratch)

Scale:
  38,000 samples, 1.4M parameters, trains in 2 minutes on CPU
  Demonstrates the complete pipeline: data → model → training → evaluation
```

### Comparison

| | SimScaleAI | OpenVLA | RT-2 |
|---|---|---|---|
| Vision encoder | ViT (2 layers) | SigLIP (24 layers) | ViT-G (40 layers) |
| Language model | Char-level (2 layers) | LLaMA-7B | PaLM-55B |
| Fusion | Concatenate + transformer | Interleaved tokens | Interleaved tokens |
| Action output | MLP → continuous | LLM → discrete tokens | LLM → discrete tokens |
| Parameters | 1.4M | 7B | 55B |
| Training data | 38K steps | 1M+ episodes | 130K episodes |
| Pre-trained? | No | Yes (SigLIP + LLaMA) | Yes (ViT-G + PaLM) |
| Zero-shot? | No | Yes | Yes |

---

## Part 11: Emergent Capabilities — What Happens at Scale

VLMs exhibit **emergent capabilities** — behaviors that appear only when the model is large enough:

### Capability 1: Spatial Reasoning

```
Small VLM (ours): Can't really reason about spatial relationships
Large VLM (RT-2): "put the apple to the LEFT of the banana" → correct placement

The model learned left/right/near/far from millions of image-text pairs
```

### Capability 2: Semantic Understanding

```
Small VLM: Treats "red block" as character patterns
Large VLM: Understands that "the scarlet cube" = "the red block"
           Understands "the thing on the left" = whatever object is on the left

The model learned synonyms and spatial references from internet text
```

### Capability 3: Chain-of-Thought Reasoning

```
Instruction: "put the blocks in order from smallest to largest"

Large VLM (internal reasoning):
  1. I see three blocks: small (blue), medium (red), large (green)
  2. Smallest to largest means: blue, red, green
  3. I need to arrange them left to right
  4. Action: pick blue, place left. Pick red, place middle. Pick green, place right.
```

### Capability 4: Task Transfer

```
Trained on: pick-and-place, drawer opening, button pressing
Never trained on: "use the sponge to wipe the table"

Large VLM: Still succeeds! It composes:
  - "sponge" → identifies sponge-shaped object in image
  - "wipe" → repetitive horizontal motion (learned from internet videos)
  - "table" → the flat surface below
```

These capabilities are why companies invest billions in scaling VLMs.

---

## Part 12: Building Your Own VLM — A Practical Roadmap

If you wanted to build a production VLM for robotics, here's the path:

### Level 1: What We've Done (SimScaleAI)

```
✅ Implement ViT vision encoder from scratch
✅ Implement character-level language encoder
✅ Fusion transformer for cross-modal attention
✅ MLP action head for robot control
✅ Training on expert demonstrations
✅ Evaluation in simulation

Time: 2 days
Cost: $0 (CPU only)
Result: Working architecture, educational demo
```

### Level 2: Add Pre-Trained Components

```
Replace char tokenizer → HuggingFace CLIP text encoder
Replace custom ViT → HuggingFace CLIP vision encoder
Keep fusion + action head

Code change:
  from transformers import CLIPModel
  clip = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
  visual_tokens = clip.vision_model(images).last_hidden_state
  text_tokens = clip.text_model(input_ids).last_hidden_state

Time: 1 day
Cost: ~$10 (need GPU for CLIP)
Result: Meaningful visual features, better language understanding
```

### Level 3: Real Camera Images

```
Add RGB camera rendering in MuJoCo:
  mujoco.Renderer(model, height=224, width=224)
  image = renderer.render()

Train on real rendered images instead of random noise
The vision encoder can now ACTUALLY see the scene

Time: 2 days
Cost: ~$50 (GPU training for 4-8 hours)
Result: Vision encoder learns meaningful representations
```

### Level 4: Scale Up

```
Use OpenVLA architecture
Fine-tune on your robot data
Deploy with action chunking (predict 8 actions at once)

Time: 2 weeks
Cost: ~$500 (A100 GPU for extended training)
Result: Production-grade VLA for your specific task
```

---

## Glossary

| Term | Definition |
|------|-----------|
| **VLM** | Vision-Language Model — understands both images and text |
| **VLA** | Vision-Language-Action — a VLM extended with robot action output |
| **ViT** | Vision Transformer — processes images using patches + transformer |
| **Patch** | A small tile of an image (e.g., 8×8 pixels) used as a "token" |
| **CLS token** | Special token prepended to a sequence that learns a global summary |
| **Embedding** | A learned dense vector representation of discrete input (characters, patches) |
| **Embedding space** | High-dimensional coordinate system where similar items cluster together |
| **Tokenization** | Converting raw text into discrete tokens (characters, subwords, or words) |
| **BPE** | Byte-Pair Encoding — a subword tokenization algorithm used by GPT/CLIP |
| **Self-attention** | Mechanism where each token computes relevance to all other tokens |
| **Cross-attention** | Attention between tokens from different modalities (vision ↔ language) |
| **Multi-head attention** | Running multiple attention computations in parallel (different perspectives) |
| **Query / Key / Value** | The three projections in attention: Q asks, K advertises, V provides info |
| **Contrastive learning** | Training by pushing matching pairs close and non-matching pairs apart |
| **CLIP** | OpenAI's contrastive VLM — matches images to text in shared embedding space |
| **SigLIP** | Google's improved CLIP — used as the vision backbone in OpenVLA |
| **Visual grounding** | Connecting language concepts to specific regions of an image |
| **Zero-shot** | Handling inputs never seen during training (generalization) |
| **Modality** | A type of input data: vision (images), language (text), proprioception (state) |
| **Modality alignment** | Making representations from different modalities compatible |
| **Pre-training** | Training on large generic data to learn general understanding |
| **Fine-tuning** | Adapting a pre-trained model to a specific task with smaller data |
| **Foundation model** | A large pre-trained model that can be adapted to many downstream tasks |
| **Action chunking** | Predicting multiple future actions at once for smoother robot control |
| **RT-2** | Google's 55B VLA — first VLM successfully deployed on real robots |
| **OpenVLA** | Stanford's open-source 7B VLA — fine-tunable for custom robot tasks |
| **PaLM / PaLI** | Google's LLM / VLM used as the backbone in RT-2 |
| **LLaMA** | Meta's open-source LLM — used as the language backbone in OpenVLA |
| **Emergent capabilities** | Behaviors that appear only when models reach sufficient scale |
| **LayerNorm** | Normalization technique that stabilizes training (mean≈0, std≈1) |
| **GELU** | Gaussian Error Linear Unit — smooth activation function used in transformers |
| **AdamW** | Optimizer combining adaptive learning rates with weight decay |
| **Cosine annealing** | Learning rate schedule that gradually decreases the learning rate |
| **MSE loss** | Mean Squared Error — average of (prediction − target)² |
